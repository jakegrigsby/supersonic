import numpy as np
import tensorflow as tf

import environment
import utils

def ppo_agent(env_id, hyp_dict, log_dir):
    """
    When tuning hyperparameters, it'll be easier to create and modify dictionaries, which can be passed between
    MPI processes more efficiently. This function takes a dictionary describing the agent's params, as well as a
    string representing the environment and a log dir, and creates a BaseAgent.
    """
    x = hyp_dict
    return BaseAgent(env_id,
                    exp_lr = x['exp_lr']
                    policy_lr = x['policy_lr']
                    val_lr = x['val_lr']
                    vis_model = models.MODEL_REGISTRY[x['vis_model']],
                    policy_model = models.MODEL_REGISTRY[x['policy_model']],
                    val_model = models.MODEL_REGISTRY[x['val_model']],
                    exp_target_model = models.MODEL_REGISTRY[x['exp_target_model']],
                    exp_train_model = models.MODEL_REGISTRY[x['exp_train_model']],
                    exp_net_opt_steps = x['exp_net_opt_steps'],
                    gamma = x['gamma'],
                    lam = x['lam'],
                    log_dir = log_dir
                    )

class BaseAgent:
    """
    Basic version of Proximal Policy Optimization (Clip) with exploration by Random Network Distillation.

    Needs a lot of testing and probably debugging. I wrote up a quick outline of the training loop.
    
    Also TODO is all the calculations of metric we want to log, and the actual logging at the end of each episode.
    """
    def __init__(self, env_id, **kwargs):
        """ TODO: default params """
        self.env = environemnt.auto_env(env_id)

        self.exp_lr = exp_lr
        self.policy_lr = policy_lr
        self.val_lr = val_lr

        self.vis_model = vis_model()
        self.policy_model = policy_model(env.action_space.shape)
        self.val_model = val_model()
        self.exp_target_model = exp_target_model()
        self.exp_train_model = exp_train_model()

        self.exp_net_opt_steps = exp_net_opt_steps

        self.gamma = gamma
        self.lam = lam

        self.log_dir = log_dir

    def train(self, rollouts, device):
        for _ in range(rollouts):
            trajectory = self.rollout(steps)
            self.update_models(trajectory, device)

    def test(self, episodes, max_ep_steps=4500):
    """
    Test the agent's performance.
    """
        for ep in range(episodes):
            step = 0
            obs, rew, done, info = self.env.reset()
            while step < max_ep_steps and not done:
                action = self.choose_action(obs)
                obs, rew, done, info = self.env.step(action)

    def rollout(steps):
        """
        Step through the environment using current policy. Calculate all the metrics needed by update_models() and save it to a util.Trajectory object
        """
        obs, e_rew, done, info = self.env.reset()
        step = 0
        trajectory = utils.Trajectory()
        while step < steps and not done:
            action_probs, val = self.choose_action_get_value(obs)
            action = np.argmax(action_probs)
            obs, e_rew, done, info = self.env.step(action)
            i_rew, exp_target = self.calc_intrinsic_reward(obs)        
            trajectory.add(obs, e_rew, i_rew, exp_target, action_probs, val) 
            step += 1
        trajectory.end_trajectory(self.gamma, self.lam)
        return trajectory

    def choose_action(self, obs):
        """
        Choose an action based on the current observation. Saves computation by not running the value net,
        which makes it a good choice for testing the agent.
        """
        features = self.vis_model(obs)
        actions = self.policy_model(features)
        return np.argmax(actions)

    def choose_action_get_value(self, obs):
        """
        Run the entire network and return the action probability distribution (not just the chosen action) as well as the values
        from the value net. Used during training -  when more information is needed.
        """
        features = self.vis_model(obs)
        action_probs = self.policy_model(features)
        val = self.val_model(features)
        return action_probs, val

    def calc_intrinsic_reward(self, state):
        """
        reward as described in Random Network Distillation
        """
        target = self.exp_target_model(state)
        pred = self.exp_train_model(state)
        rew = np.abs(target - pred)
        return rew, target #save targets to trajectory to avoid another call to the exp_target_model network during update_models()


    def update_models(self, trajectory, device):
    """
    Update the vision, policy, value, and exploration (RND) networks based on a utils.Trajectory object generated by a rollout.
    Should be able specify device so that multiple agents can be efficiently trained on the same (multi-gpu) machine.
    """
        with tf.device(device):
            #create new training set
            dataset = tf.data.Dataset.from_tensor_slices((trajectory.states, trajectory.exp_targets))
            dataset.shuffle(100)

            #update exploration net
            optimizer = tf.train.AdamOptimizer(learning_rate=self.exp_lr)
            step = 0
            for (batch, (state, target)) in enumerate(exp_dataset.take(64)):
                if step > self.exp_net_opt_steps: break
                with tf.GradientTape() as tape:
                    loss = tf.math.abs(target - self.exp_train_model(state))
                grads = tape.gradient(loss, self.exp_train_model.variables)
                optimizer.apply_gradients(zip(grads, self.exp_train_model.variables), global_step=tf.train.get_or_create_global_step())
                step += 1

            #create new training set, and send old one to garbage collection
            dataset = tf.data.Dataset.from_tensor_slices((trajectory.states, trajectory.rews, trajectory.old_act_probs, trajectory.gaes))
            dataset.shuffle(100)
            
            #update policy and value nets
            p_optimizer = tf.train.AdamOptimizer(learning_rate=self.policy_lr)
            v_optimizer = tf.train.AdamOptimizer(learning_rate=self.val_lr)
            step = 0
            for (batch, (state, rew, old_act_prob, gae)) in enumerate(dataset.take(64)):
                if step > self.policy_net_opt_steps and step > self.val_net_opt_steps: break
                with tf.GradientTape(persistent=True) as tape:
                    features = self.vis_model(state)
                    new_act_probs = self.policy_model(features)
                    val = self.value_model(features)
                    ratios = tf.exp(tf.log(new_act_probs) - tf.log(old_act_probs))
                    min_gae = tf.where(gae>0, (1+self.clip_value)*gae, (1-self.clip_value)*gae)
                    p_loss = -tf.reduce_mean(tf.minimum(ratio * gae, min_gae))
                    v_loss = tf.reduce_mean(tf.square(rew + self.gamma*  - val))

                grads = tape.gradient(p_loss, [self.vis_model.variables, self.policy_model.variables])
                p_optimizer.apply_gradients(zip(grads, [self.vis_model.variables, self.policy_model.variables]), global_step=tf.train.get_or_create_global_step())
                grads = tape.gradient(v_loss, [self.viz_model.variables, self.val_model.variables])
                v_optimizer.apply_gradients(zip(grads, [self.vis_model.variables, self.val_model.variables]))
                del tape
                step += 1 








